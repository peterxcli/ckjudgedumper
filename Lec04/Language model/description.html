<h2>Language model</h2>
<h3>Description</h3>
<div>
<p>在自然語言處理這個領域中，語言模型 (Language model) 是一個經常被使用到的工具。在諸如翻譯、問答、文本產生等等的任務當中，我們經常會需要去判斷電腦產生出來的句子是否接近人類會講出來的自然語言，因此我們需要有一種「評估一句話有多像真人講出來的話」的方法，而這個方法就是語言模型。</p><p>建構一個語言模型的步驟如下：</p><p>1. 蒐集大量的文本，這些文本可以來自報章雜誌、期刊論文、經典名著，甚至網路論壇等等。蒐集到的文本將會是許多的句子。</p><p>2. 將文本中的句子進行人工斷詞，並統計各個斷出來的詞出現的機率。這樣計算每一個詞的機率的方法，稱為1-gram。</p><p>3. 句子往往是由多個詞組成的結構。所以每一個詞出現的機率，應該和這個詞之前出現的詞有關，例如我們會說『晚餐真好吃』但比較不可能說出『天氣真好吃』，因為『好吃』不太可能用形容『天氣』。因此我們可以修改 1-gram 的方法，將一個詞 m 出現的機率看成 P (m | m之前一個字、m之前兩個字 ... m之前n-1個字)出現的條件機率。這個種根據詞之前的內容計算機率的方法就叫作 n-gram。</p><p>4. 因為文本並不能太大，所以如果n-gram 方法的 n 很大的話，每一個詞出現的機率可能都很低，因為文本搜集需要網羅各個領域的文章，幾乎不可能有重複的句子，所以一般而言，n 通常小於 4。</p><p>5. 當我們好奇一個句子出現在文本中的機率時，我們可以利用該句子的組成成份依照順序出現的條件機率，來定義該句子的出現機率。以「商品和服務」這個句子為例，我們可以利用 2-gram 以下的算式來算出這個句子的出現機率：</p><p>P(商品和服務) = P(商品|BOS) *<em> </em>P(和|商品) *<em> </em>P(服務|商品) * P(商品 | EOS)</p><p>其中，BOS 代表 Begin of Sentence。EOS 是 End of Sentence。P(商品 | BOS) 是「商品」這個詞出現在句首的機率； P(和|商品) 是「和」這個詞出現在「商品」這個詞正後面的機率。</p><p>在這個練習中，我們只要完成部分的功能，請寫出一個程式可以接受 5 個浮點數的輸入，作為一組詞的 2-gram 機率計算器 - 第一個數字為 P(第一個詞 | BOS)的，第二個詞為 P(第二個詞 | 第一個詞 ) 以此類推。並輸出這組詞的出現機率，為了精準，請輸出到小數點下 15 位。</p><hr><p>In natural language processing, language models are a frequently used tool. In tasks such as translation, question answering, text generation, etc., we often need to judge whether the sentences generated by the computer are close to the natural language that humans can speak, so we need to have a way of "evaluating how much a sentence is spoken by a real person" words" method, and this method is the language model.</p><p>Steps to change a language model:</p><ol><li><p>Collect vast amounts of text from newspapers, journal articles, classics, and even internet forums. The collected text will be many sentences.</p></li><li><p>The sentences in the text are manually segmented, and the probability of occurrence of each segmented word is counted. The method of calculating the probability of each word in this way is called 1-gram.</p></li><li><p>Sentences tend to be structures made up of multiple words. So the probability of each word appearing should be related to the words that appear before the word. For example, we will say "晚餐真好吃"(dinner is delicious) but it is less likely to say "天氣真好吃"(the weather is delicious) because "delicious(真好吃)" is unlikely to be used Describe "weather(天氣)". Therefore, we can modify the 1-gram method and regard the probability of a word m as the conditional probability of P (m | one word before m, two words before m ... n-1 words before m). The method of calculating the probability of each word in this way is called n-gram.</p></li><li><p>Because the text is not too large (compared to the real world), if the n of the n-gram method is very large, the probability of each word appearing may be very low, because text collection needs to cover articles in various fields, and it is almost impossible to have Repeated sentences, so in general, n is usually less than 4.</p></li><li><p>When we are curious about the probability of a sentence appearing in the text, we can use the conditional probability that the components of the sentence appear in order to define the probability of the sentence. Taking the sentence "商品和服務)"(goods and services) as an example, we can use the following 2-gram model to calculate the probability of occurrence of this sentence</p></li></ol><p>P(商品和服務) = P(商品|BOS) *<em> </em>P(和|商品) *<em> </em>P(服務|商品) * P(商品 | EOS)</p><p>BOS stands for Begin of Sentence. EOS is the End of Sentence. P(商品 | BOS) is the probability that the word "商品" appears at the beginning of a sentence; P(和|商品) is the probability that the word "和" appears after the word "商品".</p><p>As long as one part of the function is completed in this exercise, we can input a word that accepts 5 numbers as a probability of a group of words 2-gram, which can be written as P (the first word | BOS), the second word For P(2nd word|1st word) and so on. And output this set of probabilities, in order to indicate the word, please output to 15 decimal places.</p><p></p>
</div>
<p></p>
<h3>Input</h3>
5 consecutive decimals, representing the consecutive probability of a group of words. Each decimal is separated by a space.
ex : 0.5, 0.5, 0.5, 0.5, 0.5

連續  5 個小數，代表一組詞的連續機率。每一個小數都會被一個空格間隔。
ex : 0.5, 0.5, 0.5, 0.5, 0.5

<p></p>
<h3>Output</h3>
The probability of this group of words.
ex : 0.031250000000000

這組詞出現的機率。
ex : 0.031250000000000
<p></p>
<h3>Loader Code</h3>
<div>
<p>Your code will be judge using this program:</p>
</div>
<pre>

</pre>
<div>
<h3>Sample1</h3>
<h4>Input</h4>
<pre>
0.01927393127323  0.0848232131  0.0493172832131231  0.122711212132  0.0645279821321
</pre>
<h4>Output</h4>
<pre>
0.000000638434724
</pre>
</div>
<div>
<h3>Sample2</h3>
<h4>Input</h4>
<pre>
0.4910281212  0.01228323  0.012832  0.019222923  0.1282893232
</pre>
<h4>Output</h4>
<pre>
0.000000190863657
</pre>
</div>
